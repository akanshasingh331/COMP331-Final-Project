# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pq3i9-VN3X1tQ7bbD53eVqXERANh3Azd
"""

# STEP 1: Import the main libraries we need

import pandas as pd
import numpy as np

# Optional: for simple plots later
import matplotlib.pyplot as plt

# STEP 2: Upload CSV files from your computer

from google.colab import files
uploaded = files.upload()

# STEP 3: Read the uploaded CSVs into pandas DataFrames
# CHANGE the filenames inside read_csv(...) if your names are slightly different

stores = pd.read_csv('stores data-set.csv')
features = pd.read_csv('Features data set.csv')
sales = pd.read_csv('sales data-set.csv')

# Quick check: print shapes (rows, columns)
print("Stores shape:", stores.shape)
print("Features shape:", features.shape)
print("Sales shape:", sales.shape)

# STEP 4: Look at the first few rows of each table

print("=== STORES HEAD ===")
display(stores.head())

print("=== FEATURES HEAD ===")
display(features.head())

print("=== SALES HEAD ===")
display(sales.head())

# STEP 4: Look at the first few rows of each table

print("=== STORES HEAD ===")
display(stores.head())

print("=== FEATURES HEAD ===")
display(features.head())

print("=== SALES HEAD ===")
display(sales.head())

# STEP 5: Check missing values in each table

print("=== Missing values in STORES ===")
print(stores.isnull().sum())

print("\n=== Missing values in FEATURES ===")
print(features.isnull().sum())

print("\n=== Missing values in SALES ===")
print(sales.isnull().sum())

# STEP 5: Check missing values in each table

print("=== Missing values in STORES ===")
print(stores.isnull().sum())

print("\n=== Missing values in FEATURES ===")
print(features.isnull().sum())

print("\n=== Missing values in SALES ===")
print(sales.isnull().sum())

# STEP 7A: Check if Store IDs are unique in STORES

print("Number of unique Store IDs:", stores['Store'].nunique())
print("Number of rows in Stores table:", len(stores))

duplicate_stores = stores[stores.duplicated(subset=['Store'], keep=False)]
print("\nDuplicate Store rows (if any):")
display(duplicate_stores)

# STEP 7B: Check potential key (Store, Date) in FEATURES

features_key_counts = features.groupby(['Store', 'Date']).size().reset_index(name='count')

duplicates_features = features_key_counts[features_key_counts['count'] > 1]

print("Number of (Store, Date) duplicates in FEATURES:", len(duplicates_features))
display(duplicates_features.head())

# STEP 7C: Check potential key (Store, Dept, Date) in SALES

sales_key_counts = sales.groupby(['Store', 'Dept', 'Date']).size().reset_index(name='count')

duplicates_sales = sales_key_counts[sales_key_counts['count'] > 1]

print("Number of (Store, Dept, Date) duplicates in SALES:", len(duplicates_sales))
display(duplicates_sales.head())

# STEP 8: Referential Integrity between tables (Store key)

stores_ids = set(stores['Store'].unique())
features_store_ids = set(features['Store'].unique())
sales_store_ids = set(sales['Store'].unique())

# Stores used in Features but missing in Stores table
missing_in_stores_from_features = features_store_ids - stores_ids

# Stores used in Sales but missing in Stores table
missing_in_stores_from_sales = sales_store_ids - stores_ids

print("Stores in FEATURES but not in STORES:", missing_in_stores_from_features)
print("Stores in SALES but not in STORES:", missing_in_stores_from_sales)

# STEP 9: Convert Date columns to proper datetime format

features['Date'] = pd.to_datetime(features['Date'])
sales['Date'] = pd.to_datetime(sales['Date'])

print("Features dates from", features['Date'].min(), "to", features['Date'].max())
print("Sales dates from", sales['Date'].min(), "to", sales['Date'].max())

# STEP 9: Convert Date columns to proper datetime format (DAY FIRST)

features['Date'] = pd.to_datetime(features['Date'], dayfirst=True, errors='coerce')
sales['Date'] = pd.to_datetime(sales['Date'], dayfirst=True, errors='coerce')

print("Features dates from", features['Date'].min(), "to", features['Date'].max())
print("Sales dates from", sales['Date'].min(), "to", sales['Date'].max())

Stores in FEATURES but not in STORES: set()
Stores in SALES but not in STORES: set()

# ================================
# STEP 1: Import libraries
# ================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_columns', 50)

# ================================
# STEP 2: Upload CSV files from your computer
# ================================
from google.colab import files

print("Please select your 3 CSV files: stores, features, and sales.")
uploaded = files.upload()

# Check uploaded filenames
print("\nUploaded files:")
for fn in uploaded.keys():
    print(" -", fn)



# ================================
# STEP 3: Read CSVs into DataFrames
# ================================
stores = pd.read_csv('stores data-set.csv')
features = pd.read_csv('Features data set.csv')
sales = pd.read_csv('sales data-set.csv')

print("Shapes (rows, columns):")
print("Stores:", stores.shape)
print("Features:", features.shape)
print("Sales:", sales.shape)

print("\n=== STORES HEAD ===")
display(stores.head())

print("\n=== FEATURES HEAD ===")
display(features.head())

print("\n=== SALES HEAD ===")
display(sales.head())

# ================================
# STEP 4: Basic info about each table
# ================================
print("=== STORES INFO ===")
stores.info()

print("\n=== FEATURES INFO ===")
features.info()

print("\n=== SALES INFO ===")
sales.info()

# ================================
# STEP 5: Missing values - counts & percentages
# ================================

def missing_report(df, name):
    print(f"\n=== Missing values report for {name} ===")
    total = df.isnull().sum()
    pct = (df.isnull().mean() * 100).round(2)
    report = pd.DataFrame({'missing_count': total, 'missing_pct': pct})
    display(report)

missing_report(stores, "STORES")
missing_report(features, "FEATURES")
missing_report(sales, "SALES")

# ================================
# STEP 6: Uniqueness & duplicate checks
# ================================

# 6A. STORES: Store should be unique
print("Number of unique stores:", stores['Store'].nunique())
print("Number of rows in Stores table:", len(stores))

duplicate_stores = stores[stores.duplicated(subset=['Store'], keep=False)]
print("\nDuplicate Store rows (if any):")
display(duplicate_stores)

# 6B. FEATURES: (Store, Date) should usually be unique
features_key_counts = features.groupby(['Store', 'Date']).size().reset_index(name='count')
duplicates_features = features_key_counts[features_key_counts['count'] > 1]

print("\nNumber of (Store, Date) duplicates in FEATURES:", len(duplicates_features))
display(duplicates_features.head())

# 6C. SALES: (Store, Dept, Date) should usually be unique
sales_key_counts = sales.groupby(['Store', 'Dept', 'Date']).size().reset_index(name='count')
duplicates_sales = sales_key_counts[sales_key_counts['count'] > 1]

print("\nNumber of (Store, Dept, Date) duplicates in SALES:", len(duplicates_sales))
display(duplicates_sales.head())

# ================================
# STEP 7: Referential Integrity between tables using Store
# ================================
stores_ids = set(stores['Store'].unique())
features_store_ids = set(features['Store'].unique())
sales_store_ids = set(sales['Store'].unique())

missing_in_stores_from_features = features_store_ids - stores_ids
missing_in_stores_from_sales = sales_store_ids - stores_ids

print("Stores in FEATURES but not in STORES:", missing_in_stores_from_features)
print("Stores in SALES but not in STORES:", missing_in_stores_from_sales)

# ================================
# STEP 8: Convert Date columns to proper datetime (DAY FIRST)
# ================================
features['Date'] = pd.to_datetime(features['Date'], dayfirst=True, errors='coerce')
sales['Date'] = pd.to_datetime(sales['Date'], dayfirst=True, errors='coerce')

print("Features dates from", features['Date'].min(), "to", features['Date'].max())
print("Sales dates from", sales['Date'].min(), "to", sales['Date'].max())

# ================================
# STEP 9: Validity checks
# ================================

# 9A. Negative Weekly_Sales?
negative_sales = sales[sales['Weekly_Sales'] < 0]
print("Number of rows with negative Weekly_Sales:", len(negative_sales))
display(negative_sales.head())

# 9B. Numeric ranges in FEATURES
numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',
                'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']

print("\n=== Numeric ranges in FEATURES ===")
for col in numeric_cols:
    if col in features.columns:
        print(f"\nColumn: {col}")
        print("Min:", features[col].min())
        print("Max:", features[col].max())
        print("Mean:", features[col].mean())

# ================================
# STEP 10: Merge Sales (fact) with Features and Stores (dimensions)
# ================================

# Merge SALES + FEATURES on Store, Date, IsHoliday
sales_features = pd.merge(
    sales,
    features,
    on=['Store', 'Date', 'IsHoliday'],
    how='left'
)

print("Merged Sales + Features shape:", sales_features.shape)
display(sales_features.head())

# Merge with STORES on Store
full_data = pd.merge(
    sales_features,
    stores,
    on='Store',
    how='left'
)

print("\nFinal merged dataset shape:", full_data.shape)
display(full_data.head())

# ================================
# STEP 11A: Average weekly sales by store type
# ================================
avg_sales_by_type = full_data.groupby('Type')['Weekly_Sales'].mean().reset_index()
avg_sales_by_type = avg_sales_by_type.sort_values('Weekly_Sales', ascending=False)

print("Average Weekly Sales by Store Type:")
display(avg_sales_by_type)

# Simple bar plot (optional)
avg_sales_by_type.plot(x='Type', y='Weekly_Sales', kind='bar', legend=False)
plt.xlabel('Store Type')
plt.ylabel('Average Weekly Sales')
plt.title('Average Weekly Sales by Store Type')
plt.show()

# ================================
# STEP 11B: Holiday vs Non-Holiday average sales
# ================================
holiday_summary = sales.groupby('IsHoliday')['Weekly_Sales'].agg(['count', 'mean', 'sum']).reset_index()
holiday_summary['IsHoliday'] = holiday_summary['IsHoliday'].map({False: 'Non-Holiday', True: 'Holiday'})

print("Holiday vs Non-Holiday Sales:")
display(holiday_summary)

# ================================
# STEP 11C: Percentage of missing MarkDown fields in full_data
# ================================
markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']
markdown_missing = full_data[markdown_cols].isnull().mean().round(3) * 100

print("Percentage of missing values in MarkDown fields (full merged data):")
display(markdown_missing)

# ================================
# STEP 12: Save summary tables to CSV
# ================================

missing_features = features.isnull().sum().to_frame(name='missing_count')
missing_features.to_csv('missing_features.csv')

avg_sales_by_type.to_csv('avg_sales_by_type.csv', index=False)
holiday_summary.to_csv('holiday_vs_nonholiday_sales.csv', index=False)

print("Files saved: missing_features.csv, avg_sales_by_type.csv, holiday_vs_nonholiday_sales.csv")

from google.colab import files
files.download('missing_features.csv')
files.download('avg_sales_by_type.csv')
files.download('holiday_vs_nonholiday_sales.csv')

# ============================================
# FULL PIPELINE: ANALYZE & EXPORT PDF REPORT
# ============================================

# 1) Install and import libraries
!pip install fpdf

import pandas as pd
import numpy as np
from fpdf import FPDF
from google.colab import files

# ---------- CONFIG: CHANGE FILENAMES IF NEEDED ----------
stores_file   = 'stores data-set.csv'
features_file = 'Features data set.csv'
sales_file    = 'sales data-set.csv'

# 2) Load data
stores   = pd.read_csv(stores_file)
features = pd.read_csv(features_file)
sales    = pd.read_csv(sales_file)

# 3) Basic cleaning / types

# Date conversion (day first, as we discovered earlier)
features['Date'] = pd.to_datetime(features['Date'], dayfirst=True, errors='coerce')
sales['Date']    = pd.to_datetime(sales['Date'], dayfirst=True, errors='coerce')

# 4) Helper function: missing value report
def missing_report(df):
    total = df.isnull().sum()
    pct = (df.isnull().mean() * 100).round(2)
    return pd.DataFrame({'missing_count': total, 'missing_pct': pct})

stores_missing   = missing_report(stores)
features_missing = missing_report(features)
sales_missing    = missing_report(sales)

# 5) Uniqueness / duplicates

# Stores: Store should be unique
stores_unique_count = stores['Store'].nunique()
stores_rows = len(stores)
stores_duplicates = stores[stores.duplicated(subset=['Store'], keep=False)]

# Features: (Store, Date) uniqueness
features_key_counts = features.groupby(['Store', 'Date']).size().reset_index(name='count')
features_dupes = features_key_counts[features_key_counts['count'] > 1]

# Sales: (Store, Dept, Date) uniqueness
sales_key_counts = sales.groupby(['Store', 'Dept', 'Date']).size().reset_index(name='count')
sales_dupes = sales_key_counts[sales_key_counts['count'] > 1]

# 6) Referential integrity (Store key)
stores_ids   = set(stores['Store'].unique())
features_ids = set(features['Store'].unique())
sales_ids    = set(sales['Store'].unique())

missing_in_stores_from_features = features_ids - stores_ids
missing_in_stores_from_sales    = sales_ids - stores_ids

# 7) Validity checks

# Negative Weekly_Sales?
negative_sales = sales[sales['Weekly_Sales'] < 0]
negative_sales_count = len(negative_sales)

# Date ranges
features_min_date = features['Date'].min()
features_max_date = features['Date'].max()
sales_min_date    = sales['Date'].min()
sales_max_date    = sales['Date'].max()

# Numeric ranges in features
numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',
                'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']

numeric_summary = {}
for col in numeric_cols:
    if col in features.columns:
        numeric_summary[col] = {
            'min': float(features[col].min()) if not pd.isnull(features[col].min()) else None,
            'max': float(features[col].max()) if not pd.isnull(features[col].max()) else None,
            'mean': float(features[col].mean()) if not pd.isnull(features[col].mean()) else None,
        }

# 8) Completeness highlights (top missing columns)
def top_missing_text(df_missing, table_name, top_n=5):
    df_m = df_missing[df_missing['missing_count'] > 0].sort_values('missing_pct', ascending=False)
    if df_m.empty:
        return f"For {table_name}, no missing values were found in any column.\n"
    lines = [f"Top missing columns in {table_name}:"]
    for col, row in df_m.head(top_n).iterrows():
        lines.append(f" - {col}: {row['missing_count']} missing ({row['missing_pct']}%)")
    return "\n".join(lines) + "\n"

stores_missing_text   = top_missing_text(stores_missing, "STORES")
features_missing_text = top_missing_text(features_missing, "FEATURES")
sales_missing_text    = top_missing_text(sales_missing, "SALES")

# 9) Build narrative sections for the PDF

# COMPLETENESS SECTION
completeness_section = f"""
COMPLETENESS

The warehouse consists of three core tables:
- STORES (dimension): {stores.shape[0]} rows, {stores.shape[1]} columns
- FEATURES (time and regional features): {features.shape[0]} rows, {features.shape[1]} columns
- SALES (fact table): {sales.shape[0]} rows, {sales.shape[1]} columns

Missing values were analyzed for each table.

{stores_missing_text}

{features_missing_text}

{sales_missing_text}

In the warehousing context, the high level of missingness in the MarkDown fields is critical:
these columns are closely tied to promotions and holiday events. When MarkDown values are
missing, analytical models and OLAP reports cannot reliably attribute changes in Weekly_Sales
to price or promotion changes. This reduces the completeness and business value of the
retail data warehouse.
"""

# CONSISTENCY SECTION
consistency_section = f"""
CONSISTENCY

Key consistency checks focused on uniqueness of primary keys and referential integrity.

1. Dimension key uniqueness:
   - Stores table: {stores_unique_count} unique Store IDs across {stores_rows} rows.

"""
if stores_duplicates.empty:
    consistency_section += "   No duplicate Store IDs were found in the STORES dimension.\n"
else:
    consistency_section += f"   WARNING: {len(stores_duplicates)} duplicate Store rows were found.\n"

consistency_section += f"""
2. Fact and feature grain:
   - In FEATURES, the logical grain is (Store, Date).
   - Number of (Store, Date) combinations with duplicates: {len(features_dupes)}.
   - In SALES, the logical grain is (Store, Dept, Date).
   - Number of (Store, Dept, Date) combinations with duplicates: {len(sales_dupes)}.
"""

if len(features_dupes) == 0 and len(sales_dupes) == 0:
    consistency_section += """
These results indicate that the warehouse grain is enforced consistently and there are no
duplicate fact or feature records at the expected grain. This supports consistent aggregation
across OLAP cubes.
"""
else:
    consistency_section += """
The presence of duplicated keys at the expected grain introduces inconsistency: the same store,
date, and (optionally) department appear multiple times. This can lead to double counting in
aggregations and inconsistent KPIs across reports.
"""

consistency_section += f"""

3. Referential integrity (Store key):

   - Stores in FEATURES but not in STORES: {len(missing_in_stores_from_features)}
   - Stores in SALES but not in STORES: {len(missing_in_stores_from_sales)}

"""

if len(missing_in_stores_from_features) == 0 and len(missing_in_stores_from_sales) == 0:
    consistency_section += "All store IDs used in SALES and FEATURES exist in the STORES dimension, indicating good referential integrity.\n"
else:
    consistency_section += "Some store IDs appear in transactional or feature tables but do not exist in the STORES dimension, which breaks referential integrity and leads to incomplete joins.\n"

# VALIDITY SECTION
validity_section = f"""
VALIDITY

Several checks were used to evaluate validity of the data:

1. Date validity and temporal coverage:

   - FEATURES dates range from {features_min_date} to {features_max_date}
   - SALES dates range   from {sales_min_date} to {sales_max_date}

The difference in date ranges indicates that FEATURES contains observations beyond the SALES
period. For a retail warehouse, this means that features exist for weeks where no corresponding
Weekly_Sales facts are available, which can limit time-series analyses and create misleading
comparisons if not handled properly.

2. Negative Weekly_Sales values:

   - Number of rows with Weekly_Sales < 0: {negative_sales_count}

"""

if negative_sales_count == 0:
    validity_section += "No negative sales values were found, which supports the validity of the fact data.\n"
else:
    validity_section += "Negative sales values were detected; these may represent returns or data entry errors and should be reviewed for validity.\n"

validity_section += """

3. Numeric ranges in FEATURES (potential outliers):

"""

for col, stats in numeric_summary.items():
    if stats['min'] is not None:
        validity_section += f"   - {col}: min={stats['min']:.3f}, max={stats['max']:.3f}, mean={stats['mean']:.3f}\n"

validity_section += """
Extreme values in these fields may indicate outliers or sensor errors (e.g., unrealistic
temperatures or CPI values). In the warehousing context, such values should be flagged and
possibly winsorized or corrected during the Transform step of ETL.
"""

# ACCURACY SECTION
accuracy_section = f"""
ACCURACY (IN WAREHOUSING CONTEXT)

Accuracy refers to how closely the stored values reflect real-world business events.

In this retail warehouse:

- The strong referential integrity on the Store dimension and the lack of negative sales
  values (in most cases) suggest that many recorded transactions accurately represent store-
  level activity.

- However, the missing MarkDown fields reduce the ability to accurately explain changes in
  Weekly_Sales. When promotion data is absent, analysts may misattribute demand spikes to
  other factors such as seasonality or store size.

- Differences in temporal coverage between FEATURES and SALES (FEATURES continuing beyond the
  final SALES date) can generate inaccurate conclusions if analysts assume both tables cover
  the same period. For example, rolling averages or year-over-year trends may silently mix
  periods with and without complete sales data.

To improve accuracy, ETL pipelines should:

1. Enforce aligned time windows across fact and feature tables.
2. Capture promotion/markdown data more reliably, especially around key retail holidays.
3. Introduce validation rules and business logic at load time (e.g., rejecting impossible
   values for Temperature, CPI, or Fuel_Price).
"""

# 10) Create PDF with all sections

class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 14)
        self.cell(0, 10, 'Retail Data Warehouse - Data Quality Report', ln=True, align='C')
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', align='C')

pdf = PDF()
pdf.set_auto_page_break(auto=True, margin=15)
pdf.add_page()

pdf.set_font("Arial", "", 11)

def add_section(title, text):
    pdf.set_font("Arial", "B", 12)
    pdf.multi_cell(0, 8, title)
    pdf.ln(1)
    pdf.set_font("Arial", "", 11)
    pdf.multi_cell(0, 6, text)
    pdf.ln(4)

add_section("1. COMPLETENESS", completeness_section)
add_section("2. CONSISTENCY", consistency_section)
add_section("3. VALIDITY", validity_section)
add_section("4. ACCURACY (WAREHOUSING CONTEXT)", accuracy_section)

filename = "Retail_DQ_Report.pdf"
pdf.output(filename)

print(f"\nPDF report generated: {filename}")
files.download(filename)

# ============================================
# FULL PIPELINE: ANALYZE + VISUALIZE + PDF
# ============================================

# 1) Install and import libraries
!pip install fpdf

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from fpdf import FPDF
from google.colab import files

pd.set_option('display.max_columns', 50)

# ---------- CONFIG: CHANGE FILENAMES IF NEEDED ----------
stores_file   = 'stores data-set.csv'
features_file = 'Features data set.csv'
sales_file    = 'sales data-set.csv'

# 2) Load data
stores   = pd.read_csv(stores_file)
features = pd.read_csv(features_file)
sales    = pd.read_csv(sales_file)

# 3) Basic cleaning / types
features['Date'] = pd.to_datetime(features['Date'], dayfirst=True, errors='coerce')
sales['Date']    = pd.to_datetime(sales['Date'], dayfirst=True, errors='coerce')

# 4) Helper: missing-values report
def missing_report(df):
    total = df.isnull().sum()
    pct = (df.isnull().mean() * 100).round(2)
    return pd.DataFrame({'missing_count': total, 'missing_pct': pct})

stores_missing   = missing_report(stores)
features_missing = missing_report(features)
sales_missing    = missing_report(sales)

# 5) Uniqueness / duplicates
stores_unique_count = stores['Store'].nunique()
stores_rows         = len(stores)
stores_duplicates   = stores[stores.duplicated(subset=['Store'], keep=False)]

features_key_counts = features.groupby(['Store', 'Date']).size().reset_index(name='count')
features_dupes      = features_key_counts[features_key_counts['count'] > 1]

sales_key_counts = sales.groupby(['Store', 'Dept', 'Date']).size().reset_index(name='count')
sales_dupes      = sales_key_counts[sales_key_counts['count'] > 1]

# 6) Referential integrity
stores_ids   = set(stores['Store'].unique())
features_ids = set(features['Store'].unique())
sales_ids    = set(sales['Store'].unique())

missing_in_stores_from_features = features_ids - stores_ids
missing_in_stores_from_sales    = sales_ids - stores_ids

# 7) Validity checks
negative_sales       = sales[sales['Weekly_Sales'] < 0]
negative_sales_count = len(negative_sales)

features_min_date = features['Date'].min()
features_max_date = features['Date'].max()
sales_min_date    = sales['Date'].min()
sales_max_date    = sales['Date'].max()

numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',
                'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']

numeric_summary = {}
for col in numeric_cols:
    if col in features.columns:
        numeric_summary[col] = {
            'min': float(features[col].min()) if not pd.isnull(features[col].min()) else None,
            'max': float(features[col].max()) if not pd.isnull(features[col].max()) else None,
            'mean': float(features[col].mean()) if not pd.isnull(features[col].mean()) else None,
        }

# 8) Completeness text helper
def top_missing_text(df_missing, table_name, top_n=5):
    df_m = df_missing[df_missing['missing_count'] > 0].sort_values('missing_pct', ascending=False)
    if df_m.empty:
        return f"For {table_name}, no missing values were found in any column.\n"
    lines = [f"Top missing columns in {table_name}:"]
    for col, row in df_m.head(top_n).iterrows():
        lines.append(f" - {col}: {row['missing_count']} missing ({row['missing_pct']}%)")
    return "\n".join(lines) + "\n"

stores_missing_text   = top_missing_text(stores_missing, "STORES")
features_missing_text = top_missing_text(features_missing, "FEATURES")
sales_missing_text    = top_missing_text(sales_missing, "SALES")

# 9) Build narrative sections for text

# COMPLETENESS
completeness_section = f"""
COMPLETENESS

The warehouse consists of three core tables:
- STORES (dimension): {stores.shape[0]} rows, {stores.shape[1]} columns
- FEATURES (time and regional features): {features.shape[0]} rows, {features.shape[1]} columns
- SALES (fact table): {sales.shape[0]} rows, {sales.shape[1]} columns

Missing values were analyzed for each table.

{stores_missing_text}
{features_missing_text}
{sales_missing_text}
In the warehousing context, the high level of missingness in the MarkDown fields is critical:
these columns are closely tied to promotions and holiday events. When MarkDown values are
missing, analytical models and OLAP reports cannot reliably attribute changes in Weekly_Sales
to price or promotion changes. This reduces the completeness and business value of the
retail data warehouse.
"""

# CONSISTENCY
consistency_section = f"""
CONSISTENCY

Key consistency checks focused on uniqueness of primary keys and referential integrity.

1. Dimension key uniqueness:
   - Stores table: {stores_unique_count} unique Store IDs across {stores_rows} rows.
"""
if stores_duplicates.empty:
    consistency_section += "   No duplicate Store IDs were found in the STORES dimension.\n"
else:
    consistency_section += f"   WARNING: {len(stores_duplicates)} duplicate Store rows were found.\n"

consistency_section += f"""
2. Fact and feature grain:
   - In FEATURES, the logical grain is (Store, Date).
   - Number of (Store, Date) combinations with duplicates: {len(features_dupes)}.
   - In SALES, the logical grain is (Store, Dept, Date).
   - Number of (Store, Dept, Date) combinations with duplicates: {len(sales_dupes)}.
"""
if len(features_dupes) == 0 and len(sales_dupes) == 0:
    consistency_section += """
These results indicate that the warehouse grain is enforced consistently and there are no
duplicate fact or feature records at the expected grain. This supports consistent aggregation
across OLAP cubes.
"""
else:
    consistency_section += """
The presence of duplicated keys at the expected grain introduces inconsistency: the same store,
date, and (optionally) department appear multiple times. This can lead to double counting in
aggregations and inconsistent KPIs across reports.
"""

consistency_section += f"""
3. Referential integrity (Store key):

   - Stores in FEATURES but not in STORES: {len(missing_in_stores_from_features)}
   - Stores in SALES but not in STORES: {len(missing_in_stores_from_sales)}
"""
if len(missing_in_stores_from_features) == 0 and len(missing_in_stores_from_sales) == 0:
    consistency_section += "All store IDs used in SALES and FEATURES exist in the STORES dimension, indicating good referential integrity.\n"
else:
    consistency_section += "Some store IDs appear in transactional or feature tables but do not exist in the STORES dimension, which breaks referential integrity and leads to incomplete joins.\n"

# VALIDITY
validity_section = f"""
VALIDITY

Several checks were used to evaluate validity of the data:

1. Date validity and temporal coverage:

   - FEATURES dates range from {features_min_date} to {features_max_date}
   - SALES dates range   from {sales_min_date} to {sales_max_date}

The difference in date ranges indicates that FEATURES contains observations beyond the SALES
period. For a retail warehouse, this means that features exist for weeks where no corresponding
Weekly_Sales facts are available, which can limit time-series analyses and create misleading
comparisons if not handled properly.

2. Negative Weekly_Sales values:

   - Number of rows with Weekly_Sales < 0: {negative_sales_count}
"""
if negative_sales_count == 0:
    validity_section += "No negative sales values were found, which supports the validity of the fact data.\n"
else:
    validity_section += "Negative sales values were detected; these may represent returns or data entry errors and should be reviewed for validity.\n"

validity_section += """

3. Numeric ranges in FEATURES (potential outliers):
"""
for col, stats in numeric_summary.items():
    if stats['min'] is not None:
        validity_section += f"   - {col}: min={stats['min']:.3f}, max={stats['max']:.3f}, mean={stats['mean']:.3f}\n"

validity_section += """
Extreme values in these fields may indicate outliers or sensor errors (e.g., unrealistic
temperatures or CPI values). In the warehousing context, such values should be flagged and
corrected during the Transform step of ETL.
"""

# ACCURACY
accuracy_section = f"""
ACCURACY (IN WAREHOUSING CONTEXT)

Accuracy refers to how closely the stored values reflect real-world business events.

In this retail warehouse:

- The strong referential integrity on the Store dimension and the enforced grain in the fact
  and feature tables suggest that many recorded transactions accurately represent store-level
  activity.

- However, the missing MarkDown fields reduce the ability to accurately explain changes in
  Weekly_Sales. When promotion data is absent, analysts may misattribute demand spikes to
  other factors such as seasonality or store size.

- Differences in temporal coverage between FEATURES and SALES (FEATURES continuing beyond the
  final SALES date) can generate inaccurate conclusions if analysts assume both tables cover
  the same period.

To improve accuracy, ETL pipelines should:
1. Enforce aligned time windows across fact and feature tables.
2. Capture promotion/markdown data more reliably, especially around key retail holidays.
3. Introduce validation rules and business logic at load time (e.g., rejecting impossible
   values for Temperature, CPI, or Fuel_Price).
"""

# ====================================================
# 10) VISUALIZATIONS (charts saved as PNG files)
# ====================================================

# 10.1 Missingness of MarkDown1–5
markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']
markdown_missing_pct = features[markdown_cols].isnull().mean() * 100

plt.figure(figsize=(6,4))
markdown_missing_pct.plot(kind='bar')
plt.ylabel('Missing (%)')
plt.title('Missingness in MarkDown1–5 (FEATURES)')
plt.tight_layout()
plt.savefig('chart_markdown_missing.png', dpi=150)
plt.close()

# 10.2 Holiday vs Non-Holiday average Weekly_Sales
holiday_summary = sales.groupby('IsHoliday')['Weekly_Sales'].mean().reset_index()
holiday_summary['IsHoliday'] = holiday_summary['IsHoliday'].map({False: 'Non-Holiday', True: 'Holiday'})

plt.figure(figsize=(6,4))
holiday_summary.set_index('IsHoliday')['Weekly_Sales'].plot(kind='bar')
plt.ylabel('Average Weekly Sales')
plt.title('Average Weekly Sales: Holiday vs Non-Holiday')
plt.tight_layout()
plt.savefig('chart_holiday_sales.png', dpi=150)
plt.close()

# 10.3 Weekly_Sales distribution (histogram, clipped)
plt.figure(figsize=(6,4))
sales['Weekly_Sales'].clip(lower=sales['Weekly_Sales'].quantile(0.01),
                           upper=sales['Weekly_Sales'].quantile(0.99)).hist(bins=30)
plt.xlabel('Weekly_Sales (clipped 1–99 percentile)')
plt.ylabel('Frequency')
plt.title('Distribution of Weekly_Sales')
plt.tight_layout()
plt.savefig('chart_sales_hist.png', dpi=150)
plt.close()

# ====================================================
# 11) Build PDF with text + charts
# ====================================================

class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 14)
        self.cell(0, 10, 'Retail Data Warehouse - Data Quality Report', ln=True, align='C')
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', align='C')

pdf = PDF()
pdf.set_auto_page_break(auto=True, margin=15)
pdf.add_page()

pdf.set_font("Arial", "", 11)

def add_section(title, text):
    pdf.set_font("Arial", "B", 12)
    pdf.multi_cell(0, 8, title)
    pdf.ln(1)
    pdf.set_font("Arial", "", 11)
    pdf.multi_cell(0, 6, text)
    pdf.ln(4)

# Text sections
add_section("1. COMPLETENESS", completeness_section)
add_section("2. CONSISTENCY", consistency_section)
add_section("3. VALIDITY", validity_section)
add_section("4. ACCURACY (WAREHOUSING CONTEXT)", accuracy_section)

# Chart pages
pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, "Figure 1: Missingness in MarkDown1–5", ln=True)
pdf.image('chart_markdown_missing.png', x=15, y=30, w=180)

pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, "Figure 2: Average Weekly Sales - Holiday vs Non-Holiday", ln=True)
pdf.image('chart_holiday_sales.png', x=15, y=30, w=180)

pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, "Figure 3: Distribution of Weekly_Sales (clipped)", ln=True)
pdf.image('chart_sales_hist.png', x=15, y=30, w=180)

filename = "Retail_DQ_Report_with_charts.pdf"
pdf.output(filename)

print(f"\nPDF report generated: {filename}")
files.download(filename)

# ====================================================
# FIX FOR UNICODE: CLEAN ALL TEXT BEFORE WRITING TO PDF
# ====================================================

def clean_text(s):
    """
    Converts text to pure ASCII by:
    - Removing or replacing unicode dashes
    - Replacing smart quotes
    - Dropping characters not supported by Latin-1
    """
    if not isinstance(s, str):
        s = str(s)

    replacements = {
        "–": "-",  # EN DASH
        "—": "-",  # EM DASH
        "’": "'",  # smart apostrophe
        "‘": "'",  # smart opening apostrophe
        "“": "\"", # smart quotes
        "”": "\"",
        "•": "-",  # bullet
        "→": "->", # arrow
    }
    for old, new in replacements.items():
        s = s.replace(old, new)

    # Encode to ascii and ignore unreadable chars
    s = s.encode("latin-1", "ignore").decode("latin-1")
    return s


# Clean all text sections
completeness_section_clean = clean_text(completeness_section)
consistency_section_clean  = clean_text(consistency_section)
validity_section_clean     = clean_text(validity_section)
accuracy_section_clean     = clean_text(accuracy_section)


# ====================================================
# 11) Build PDF with text + charts (SAFELY)
# ====================================================

class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 14)
        header_title = clean_text('Retail Data Warehouse - Data Quality Report')
        self.cell(0, 10, header_title, ln=True, align='C')
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        footer_text = clean_text(f'Page {self.page_no()}')
        self.cell(0, 10, footer_text, align='C')


pdf = PDF()
pdf.set_auto_page_break(auto=True, margin=15)
pdf.add_page()
pdf.set_font("Arial", "", 11)

def add_section(title, text):
    pdf.set_font("Arial", "B", 12)
    pdf.multi_cell(0, 8, clean_text(title))
    pdf.ln(1)
    pdf.set_font("Arial", "", 11)
    pdf.multi_cell(0, 6, clean_text(text))
    pdf.ln(4)

# Add cleaned text sections
add_section("1. COMPLETENESS", completeness_section_clean)
add_section("2. CONSISTENCY", consistency_section_clean)
add_section("3. VALIDITY", validity_section_clean)
add_section("4. ACCURACY (WAREHOUSING CONTEXT)", accuracy_section_clean)

# Add charts
pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, clean_text("Figure 1: Missingness in MarkDown1-5"), ln=True)
pdf.image('chart_markdown_missing.png', x=15, y=30, w=180)

pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, clean_text("Figure 2: Average Weekly Sales - Holiday vs Non-Holiday"), ln=True)
pdf.image('chart_holiday_sales.png', x=15, y=30, w=180)

pdf.add_page()
pdf.set_font("Arial", "B", 12)
pdf.cell(0, 8, clean_text("Figure 3: Distribution of Weekly_Sales (clipped)"), ln=True)
pdf.image('chart_sales_hist.png', x=15, y=30, w=180)

# Output file
filename = "Retail_DQ_Report_with_charts_FIXED.pdf"
pdf.output(filename)

print(f"\nPDF report generated successfully: {filename}")
files.download(filename)